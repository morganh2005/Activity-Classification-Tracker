# -*- coding: utf-8 -*-
"""ELEC 292 Project code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B_LL4s6CzKqI9NqkGfYYfIETg-wmwb5v
"""

# Data Storage
#Upload CSV files in a zip
# Install required libraries
!pip install h5py pandas numpy

import h5py
import pandas as pd
import numpy as np
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from google.colab import files
import os
import zipfile

# Upload and Extract CSV Files
uploaded_zip = files.upload()
zip_path = list(uploaded_zip.keys())[0]
extract_path = "/content/extracted_csv_files"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

nested_path = os.path.join(extract_path, os.listdir(extract_path)[0])
csv_files = [os.path.join(nested_path, file) for file in os.listdir(nested_path) if file.endswith('.csv')]

# Parse File Names
def parse_filename(file_name):
    parts = os.path.basename(file_name).replace('.csv', '').split('_')
    return parts[0], parts[1].lower(), parts[2], parts[3], parts[4] if len(parts) > 4 else 'Unknown'

# Create HDF5 File Structure
hdf5_path = "/content/accelerometer_data.h5"
with h5py.File(hdf5_path, 'w') as hdf:
    hdf.create_group('Raw data')
    hdf.create_group('Pre-processed data')
    segmented_group = hdf.create_group('Segmented data')
    segmented_group.create_group('Train')
    segmented_group.create_group('Test')

# Store Raw Data in HDF5
def store_raw_data(file_path, member, action, pocket, orientation, speed):
    df = pd.read_csv(file_path)

    # Check if CSV data is empty
    if df.empty:
        print(f" Warning: {file_path} is empty. Skipping.")
        return

    # Standardize column names
    df.columns = ["Time", "Acc_X", "Acc_Y", "Acc_Z", "Absolute_Acc"]

    print(f" Storing {file_path} with {df.shape[0]} rows and {df.shape[1]} columns.")

    with h5py.File(hdf5_path, 'a') as hdf:
        path = f'Raw data/{member}/{action}/{pocket}/{orientation}/{speed}'
        group = hdf.require_group(path)

        if "accelerometer_data" in group:
            del group["accelerometer_data"]

        # Store data
        group.create_dataset("accelerometer_data", data=df.values)
        group.attrs["columns"] = ",".join(df.columns)

for file_path in csv_files:
    store_raw_data(file_path, *parse_filename(file_path))

# Segment Data into Windows
def segment_data(df, window_size=5, sampling_rate=50):
    window_length = window_size * sampling_rate
    return np.array([df.iloc[i:i + window_length].values for i in range(0, len(df) - window_length + 1, window_length)])

# Load Data and Segment
action_segments = {'walking': [], 'jumping': []}

with h5py.File(hdf5_path, 'r') as hdf:
    raw_group = hdf['Raw data']
    for member in raw_group:
        for action in raw_group[member]:
            for pocket in raw_group[member][action]:
                for orientation in raw_group[member][action][pocket]:
                    for speed in raw_group[member][action][pocket][orientation]:
                        group = raw_group[member][action][pocket][orientation][speed]
                        dataset = group['accelerometer_data']
                        data = dataset[:]  # Extract stored data
                        columns = group.attrs["columns"].split(",")

                        if data.size == 0:
                            print(f" Warning: No data stored for {member} - {action} - {pocket}.")
                        else:
                            print(f" Retrieved {data.shape[0]} rows from {member} - {action} - {pocket}.")
                            df = pd.DataFrame(data, columns=columns)
                            print(df.head())  # Print first few rows to verify


# Split Data into Training and Testing
def train_test_split_data(action_segments_dict):
    x, y = [], []
    for action, segments in action_segments_dict.items():
        label = 0 if action == 'walking' else 1
        x.extend(segments)
        y.extend([label] * len(segments))

    x, y = shuffle(np.array(x), np.array(y), random_state=42)
    split_index = int(len(x) * 0.9)
    return x[:split_index], y[:split_index], x[split_index:], y[split_index:]

x_train, y_train, x_test, y_test = train_test_split_data(action_segments)

# Store Segmented Data in HDF5
with h5py.File(hdf5_path, 'a') as hdf:
    train_group = hdf['Segmented data/Train']
    test_group = hdf['Segmented data/Test']

    for dataset in ['x_train', 'y_train', 'x_test', 'y_test']:
        if dataset in train_group:
            del train_group[dataset]
        if dataset in test_group:
            del test_group[dataset]

    train_group.create_dataset('x_train', data=x_train)
    train_group.create_dataset('y_train', data=y_train)
    test_group.create_dataset('x_test', data=x_test)
    test_group.create_dataset('y_test', data=y_test)

files.download(hdf5_path)

# Visualization code for ELEC 292 Project (Google Colab compatible)
import h5py
import numpy as np
import matplotlib.pyplot as plt
import os
from google.colab import files
from collections import defaultdict

# Upload file manually through Google Colab upload tool
uploaded = files.upload()
file_path = list(uploaded.keys())[0]  # Get uploaded filename

# Load the HDF5 file
h5file = h5py.File(file_path, "r")

# Define sample paths to visualize (2 walking + 2 jumping per member)
sample_paths = {
    "Andrew": {
        "walking": [
            "Raw data/Andrew/walking/BackPocket/Sideways/Brisk/accelerometer_data",
            "Raw data/Andrew/walking/Belt/Upright/Slow/accelerometer_data"
        ],
        "jumping": [
            "Raw data/Andrew/jumping/BackPocket/Sideways/MediumHops (3)/accelerometer_data",
            "Raw data/Andrew/jumping/Belt/Upright/LightHops (3)/accelerometer_data"
        ]
    },
    "Daniel": {
        "walking": [
            "Raw data/Daniel/walking/BackPocket/Upright/Slow/accelerometer_data",
            "Raw data/Daniel/walking/Belt/UpsideDown/Normal (1)/accelerometer_data"
        ],
        "jumping": [
            "Raw data/Daniel/jumping/BackPocket/Upright/LightHops (1)/accelerometer_data",
            "Raw data/Daniel/jumping/Belt/UpsideDown/HighHops (1)/accelerometer_data"
        ]
    },
    "Morgan": {
        "walking": ["Raw data/Morgan/walking/BackPocket/UpsideDown/Normal/accelerometer_data",
            "Raw data/Morgan/walking/Belt/Sideway/Brisk/accelerometer_data"
        ],
        "jumping": [
            "Raw data/Morgan/jumping/BackPocket/UpsideDown/HighJumps (3)/accelerometer_data",
            "Raw data/Morgan/jumping/Belt/Sideways/MediumHops (3)/accelerometer_data"
        ]
    }
}

# Containers for all data
all_data = {"walking": [], "jumping": []}

# Function to plot acceleration vs time for a given dataset
def plot_acceleration(data, title):
    time = data[:, 0]
    x = data[:, 1]
    y = data[:, 2]
    z = data[:, 3]

    plt.figure(figsize=(10, 4))
    plt.plot(time, x, label='X')
    plt.plot(time, y, label='Y')
    plt.plot(time, z, label='Z')
    plt.title(title)
    plt.xlabel("Time (s)")
    plt.ylabel("Acceleration (m/s²)")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Loop through and plot each selected sample
for person, activities in sample_paths.items():
    for activity, paths in activities.items():
        for path in paths:
            data = h5file[path][:]
            all_data[activity].append(data)
            parts = path.split("/")[2:-1]  # Skip "Raw data", person, and "accelerometer_data"
            title = f"{person} - {' '.join(parts)}"
            plot_acceleration(data, title)

# --- Extra Visualization 1: Histogram of Acceleration Magnitude ---
plt.figure(figsize=(8, 4))
for activity, color, label in zip(['walking', 'jumping'], ['blue', 'red'], ['Walking', 'Jumping']):
    magnitudes = [np.sqrt(np.sum(d[:, 1:4]**2, axis=1)) for d in all_data[activity]]
    magnitudes = np.concatenate(magnitudes)
    plt.hist(magnitudes, bins=50, alpha=0.6, label=label, color=color)

plt.title("Distribution of Absolute Acceleration")
plt.xlabel("Acceleration (m/s²)")
plt.ylabel("Frequency")
plt.legend()
plt.tight_layout()
plt.show()

# --- Extra Visualization 2: Scatter Plot X vs Y Acceleration ---
plt.figure(figsize=(8, 6))
for activity, color, label in zip(['walking', 'jumping'], ['blue', 'red'], ['Walking', 'Jumping']):
    xy = np.concatenate([d[:, 1:3] for d in all_data[activity]], axis=0)
    plt.scatter(xy[:, 0], xy[:, 1], alpha=0.2, s=10, label=label, color=color)

plt.title("Scatter Plot of Acc_X vs. Acc_Y")
plt.xlabel("Acc_X (m/s²)")
plt.ylabel("Acc_Y (m/s²)")
plt.xlim(-50, 50)
plt.ylim(-50, 50)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

h5file.close()

#preprossesing
def preprocess_data(df, window_size=25):
    # Fill missing values using forward fill
    df_filled = df.fillna(method='ffill').copy()

    # Dynamically adjust window if segment is too short
    actual_window = min(window_size, len(df_filled))

    # Apply moving average filter to each axis
    for col in ['Acc_X', 'Acc_Y', 'Acc_Z', 'Absolute_Acc']:
        df_filled[col] = df_filled[col].rolling(window=actual_window, min_periods=1).mean()

    return df_filled

# Feature Extraction
import numpy as np
import pandas as pd
from scipy.stats import skew
from sklearn.preprocessing import StandardScaler

def extract_features(segments):
    feature_list = []
    for segment in segments:
        df = pd.DataFrame(segment, columns=["Time", "Acc_X", "Acc_Y", "Acc_Z", "Absolute_Acc"])
        features = []
        for col in ['Acc_X', 'Acc_Y', 'Acc_Z', 'Absolute_Acc']:
            signal = df[col]
            features.extend([
                np.mean(signal),
                np.std(signal),
                np.min(signal),
                np.max(signal),
                np.median(signal),
                np.max(signal) - np.min(signal),
                np.var(signal),
                skew(signal),
                np.sum(signal ** 2),
                np.mean(np.abs(signal))
            ])
        feature_list.append(features)
    return np.array(feature_list)

def normalize_features(x_train_features, x_test_features):
    scaler = StandardScaler()
    x_train_norm = scaler.fit_transform(x_train_features)
    x_test_norm = scaler.transform(x_test_features)
    return x_train_norm, x_test_norm

# Training and Testing Classifier
with h5py.File(hdf5_path, 'r') as hdf:
  x_train = hdf["Segmented data/Train/x_train"][:]
  y_train = hdf["Segmented data/Train/y_train"][:]
  x_test = hdf["Segmented data/Test/x_test"][:]
  y_test = hdf["Segmented data/Test/y_test"][:]

x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

scaler = StandardScaler()

# Define the classifier and the pipeline
l_reg = LogisticRegression(max_iter=10000)
clf = make_pipeline(StandardScaler(), l_reg)

# Training
clf.fit(x_train, y_train)

# Obtain the predictions and probabilities
y_pred = clf.predict(x_test)
y_clf_prob = clf.predict_proba(x_test)
print('y_pred is:', y_pred)
print('y_clf_prob is:', y_clf_prob)


with h5py.File(hdf5_path, 'r') as hdf:
    dataset_path = "Pre-processed data/Daniel/walking/Handheld/Sideway/Brisk/accelerometer_data"
    sample_data = hdf[dataset_path][:]

# Convert to DataFrame
df_sample = pd.DataFrame(sample_data)

# Segment the data and flatten
segmentSample = segment_data(df_sample)
print(segmentSample.shape)
if len(segmentSample.shape) == 3:
    n_segments = segmentSample.shape[0]
    flattened_segments = segmentSample.reshape(n_segments, -1)

predictions = clf.predict(flattened_segments)
print("test predictions: ", predictions)

activity_labels = ['walking', 'jumping']
human_readable = [activity_labels[pred] for pred in predictions]
walking_count = list(predictions).count(0)
jumping_count = list(predictions).count(1)
dominant_activity = 'walking' if walking_count > jumping_count else 'jumping'
print("I think this is", dominant_activity, "data!")

# save the model
import pickle
with open('accelerometer_model.pkl', 'wb') as f:
  pickle.dump(clf, f, protocol=pickle.HIGHEST_PROTOCOL)

# obtaining classification accuracy
acc = accuracy_score(y_test, y_pred)
print("Classification Accuracy: ", acc)

# obtaining the classification recall
recall = recall_score(y_test, y_pred)
print("Classification Recall: ", recall)

# plotting the confusion matrix
cm = confusion_matrix(y_test, y_pred)
cm_display = ConfusionMatrixDisplay(cm).plot()
plt.show()

# plot the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_clf_prob[:,1], pos_label=clf.classes_[1])
roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()
plt.show()

auc = roc_auc_score(y_test, y_clf_prob[:,1])
print('the AUC is: ', auc)

# save ROC data into a DataFrame
roc_train_data = pd.DataFrame({
    "False Positive Rate (FPR)": fpr,
    "True Positive Rate (TPR)": tpr,
    "Thresholds": thresholds
})

# save that to CSV
roc_train_data.to_csv("roc_train_data.csv", index="False")

# Download
files.download(hdf5_path)
files.download("roc_train_data.csv")
files.download('accelerometer_model.pkl')

# GUI
import sys

from PyQt5 import QtWidgets
from PyQt5.QtWidgets import QApplication, QDialog, QFileDialog
from PyQt5.uic import loadUi

import pickle

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas

class MainWindow(QDialog):
    def __init__(self):
        super().__init__()
        loadUi("gui.ui", self)

        # Window size control
        self.setFixedSize(800, 600)
        self.browse.clicked.connect(self.browseFiles)

        try:
            with open('accelerometer_model.pkl', 'rb') as f:
                self.model = pickle.load(f)
            print("Pipeline loaded successfully. Steps:", self.model.named_steps.keys())
        except Exception as e:
            print(f"Pipeline loading failed: {e}")
            self.model = None

        self.canvas = self.Canvas(self)
        self.plotLayout.addWidget(self.canvas)

    class Canvas(FigureCanvas):
        def __init__(self, parent=None):
            self.fig, self.ax = plt.subplots(figsize=(5, 4), dpi=200)
            super().__init__(self.fig)
            self.setParent(parent)
            # Adjust font sizes
            self.ax.set_title("Accelerometer Data", fontsize=8)  # Title font size
            self.ax.set_xlabel("Time", fontsize=5)  # X-axis label font size
            self.ax.set_ylabel("Acceleration", fontsize=5)  # Y-axis label font size

            # Adjust tick font sizes
            self.ax.tick_params(axis='both', which='major', labelsize=5)
            self.ax.grid(True)
            self.ax.legend()

        def plot_data(self, df, title):
            self.ax.clear()
            time = range(len(df))
            x = df[:, 1]
            y = df[:, 2]
            z = df[:, 3]

            self.ax.plot(time, x, label="x")
            self.ax.plot(time, y, label="y")
            self.ax.plot(time, z, label="z")
            self.ax.set_title(f"{title} Accelerometer Data", fontsize=8)  # Title font size
            self.ax.set_xlabel("Time", fontsize=5)  # X-axis label font size
            self.ax.set_ylabel("Acceleration", fontsize=5)  # Y-axis label font size
            self.ax.legend()
            self.draw()

    def browseFiles(self):
        file_path, _ =QFileDialog.getOpenFileName(self, 'Open File', '', '(*.csv)')
        if file_path:
            print(file_path)
            title = self.processCSV(file_path)
            self.plot_csv(file_path, title)

    def plot_csv(self, file_path, title):
        try:
            df = pd.read_csv(file_path)
            df = self.preprocess_data(df)
            self.canvas.plot_data(df.iloc[:len(df)].values, title=title)
        except Exception as e:
            if hasattr(self, 'statusLabel'):
                self.statusLabel.setText(f"Error: {str(e)}")
            print(f"Error loading CSV: {e}")

    # must apply same preprocessing and segmentation to the csv file
    def preprocess_data(self, df):
        window_size = 25
        df.columns = ['Time', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Absolute_Acc']
        df_filled = df.fillna(method='ffill').copy()
        actual_window = min(window_size, len(df_filled))
        for col in ['Time', 'Acc_X', 'Acc_Y', 'Acc_Z', 'Absolute_Acc']:
            df_filled[col] = df_filled[col].rolling(window=actual_window, min_periods=1).mean()
        return df_filled

    def segment_data(self, df):
        window_size, sampling_rate = 5, 50
        window_length = window_size * sampling_rate
        return np.array([df.iloc[i:i + window_length].values for i in range(0, len(df) - window_length + 1, window_length)])


    def processCSV(self, file_path):
        data = pd.read_csv(file_path)
        print("I have copied the data")
        processed_data = self.preprocess_data(data)
        print("I have processed the data")
        segments = self.segment_data(processed_data)
        print("I have segmented the data")
        if len(segments.shape) == 3:
            n_segments = segments.shape[0]
            flattened_segments = segments.reshape(n_segments, -1)

        predictions = self.model.predict(flattened_segments)
        print("I have predicted the data")
        print(predictions)

        activity_labels = ['walking', 'jumping']
        human_readable = [activity_labels[pred] for pred in predictions]

        # Display results
        print("Predicted activities:", human_readable)
        if hasattr(self, 'resultLabel'):  # If you have a QLabel for results
            self.resultLabel.setText(f"Activities: {', '.join(human_readable)}")

        walking_count = list(predictions).count(0)
        jumping_count = list(predictions).count(1)
        dominant_activity = 'Walking' if walking_count > jumping_count else 'Jumping'
        return dominant_activity

app = QApplication(sys.argv)
app.setApplicationName("ELEC 292 Accelerometer Classifier")
main = MainWindow()
widget = QtWidgets.QStackedWidget()
widget.addWidget(main)
widget.setFixedWidth(800)
widget.setFixedHeight(600)
widget.show()
sys.exit(app.exec_())